---
title: "Blossom Map"
author: "Alan Millington"
date: "24/03/2021"
output: html_document
---

```{r render-output-doc}
 # render("twitter-blossom-watch-2021.Rmd", output_file = 'twitter-blossom-watch-2021.html')
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, include = FALSE)
#bbox_cord
#c(-0.170246, -0.034078, -0.034078, -0.170246, 51.564585, 51.564585, 51.6112184, 51.6112184)

ts <- blossom2021 %>% filter(!is.na(bbox_coords)) %>%
    select(bbox_coords)

#library(openxlsx)

#write.xlsx(blossom2021_geo,"test_bb_geo.xlsx")

```

## Geo-Location of #Blossom Tweets

```{r echo=FALSE, fig.height=12, fig.width=9, message=FALSE, warning=FALSE}
blossom2021 %>%
  mutate(bbox_coords = gsub("\\)|c\\(", "", bbox_coords)) %>%
  separate(bbox_coords, c("box1", "box2","box3","box4", "box5", "box6", "box7", "box8"), sep = ", ") %>%
  mutate_at(c("box1", "box2","box3","box4", "box5", "box6", "box7", "box8"), as.numeric) %>%
    mutate(bbx_long = (box1 + box2 + box3 + box4)/4) %>%
    mutate(bbx_lat = (box5 + box6 + box7 + box8)/4) %>%
  filter(!is.na(place_type)) %>%
  filter(!is.na(media_url)) %>%
  leaflet() %>% 
      setView(lat = 52.8781, lng = -2.8360, zoom = 6) %>%
      addTiles() %>%
          addProviderTiles(providers$Esri.WorldImagery) %>%
 	        addCircleMarkers(lng = ~bbx_long, lat = ~bbx_lat, popup =  ~text, color = "#FF0000", radius = 3, stroke = TRUE, fillOpacity = 0.2)
```

```{r}
# Get the list of files
#----------------------------#
folder <- "twitter-blossom-watch-2021_files\\figure-gfm"
fileList <- dir(folder, recursive=TRUE)  # grep through these, if you are not loading them all

# use platform appropriate separator
files <- paste(folder, fileList, sep=.Platform$file.sep)


# Load them in
#----------------------------#
# Method 1:
invisible(sapply(files, source, local=TRUE))

#-- OR --#

# Method 2:
sapply(files, function(f) eval(parse(text=f)))

```





```{r}
news <- function(term) {
  require(dplyr)
  require(xml2)
  require(rvest)
  require(urltools)
  
  #Last 30 days
  html_dat <- read_html(paste0("https://news.google.com/search?q=",term,"%20when%3A30d&hl=en-GB&gl=GB&ceid=GB%3Aen"))
  filename <- paste(term, Sys.Date(),".xlsx",sep="")

  dat <- data.frame(Link = html_dat %>%
                      html_nodes('.VDXfz') %>% 
                      html_attr('href')) %>% 
    mutate(Link = gsub("./articles/","https://news.google.com/articles/",Link))
  
  news_dat <- data.frame(
    Title = html_dat %>%
      html_nodes('.DY5T1d') %>% 
      html_text(),
    Link = dat$Link,
    Description =  html_dat %>%
      html_nodes('.Rai5ob') %>% 
      html_text()
  ) 
  
  # Extract Source and Time (To avoid missing content)
  prod <- html_nodes(html_dat, ".SVJrMe")
  Source <- lapply(prod, function(x) {
    norm <- tryCatch(html_node(x, "a") %>% html_text() ,
                     error=function(err) {NA})
  })
  
  time <- lapply(prod, function(x) {
    norm <- tryCatch(html_node(x, "time") %>% html_text(),
                     error=function(err) {NA})
  })
  
  mydf <- data.frame(Source = do.call(rbind, Source), Time = do.call(rbind, time), stringsAsFactors = F)
  dff <- cbind(news_dat, mydf) %>% distinct(Time, .keep_all = TRUE)
  search_term <- term
  return(dff)
}
# term need to be URL encoded for use in the URL structure. Decode and clean up for the filename useage
# re-encode for the parameter passing to the function
search_term <- URLdecode('national%20trust%20blossom')
blossom_news <- news(URLencode(search_term))
```
